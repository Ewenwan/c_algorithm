# DL 知乎笔记

:octocat:

## DL中的优化算法与实现

### 梯度下降

* 如果把学习率$η$设置的过大，就会出现NaN的情况，这是因为泰勒展开中的高阶项不能被忽略。

* 当$ϵ=-ηf'(x)$当$x$选择一个比较大的数值时，其导数也比较大，$ϵ$就比较大，训练也会出问题

* 所以：**学习率大小选择**、**初始化参数选择**非常重要

### 学习率η

* 学习率太小，收敛很慢

* 学习率过大，会出现震荡，甚至出现NaN的情况

### 随机梯度下降

* 随机梯度是对梯度下降的一个无偏估计

$$
E[∂f_i(x)/∂x] = 1/n ∑_{i=1}^{n}∂f_i(x)/∂x = ∂f(x)/∂x
$$

### Mini-Batch SGD

均匀采样训练数据构成的小的Batch，然后基于这个Batch的样本进行梯度估计。

### 动量法

更多优化方法，可以参考DL这本书上的第七章。